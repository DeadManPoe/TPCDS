import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.cassandra.CassandraSQLContext
import org.apache.spark.sql.Column

import com.datastax.spark.connector._
import com.datastax.spark.connector.cql.CassandraConnector





val conf = new SparkConf();
sc.stop();
val sc = new SparkContext(conf);
val cc = new CassandraSQLContext(sc);



val keyspaceOut = "ksout"
val resultTable = "resultable7"

val variation = 0.1;

val keyspaceInput = "database20161214142404868"

cc.setKeyspace(keyspaceInput)

val fraudsters = cc.cassandraSql(
 "SELECT tp.id, tp.firstname, tp.lastname, tp.gender, tp.birthdate, tp.birthdepartment, tp.birthcommune, " + 
"d1.taxdeclaration, d1.declarationdate, d1.income, d2.taxdeclaration, d2.declarationdate, d2.income " +
                "FROM declare d1 " +
                "JOIN declare d2 ON d1.taxpayer = d2.taxpayer " +
                "JOIN taxpayer tp ON d1.taxpayer = tp.id").rdd



val finalRes=fraudsters.map{row => row.schema}.distinct.collect
val finalSorted=fraudsters.sortBy( _.getDouble(12), false) take 100
finalSorted foreach { row => println (row.getDouble(12) ) }

exit

